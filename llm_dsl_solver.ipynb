{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Based DSL Program Search for ARC-AGI-2\n",
    "\n",
    "This notebook implements an LLM-based approach to search through DSL (Domain Specific Language) programs and apply them to ARC (Abstraction and Reasoning Corpus) tasks.\n",
    "\n",
    "## Overview\n",
    "- Uses an open-source LLM to analyze ARC tasks\n",
    "- Searches through available DSL programs (originally designed for ARC-AGI-1)\n",
    "- Sequentially applies DSL transformations based on task requirements\n",
    "- Supports both training and evaluation datasets from ARC-AGI-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set these paths before running on Kaggle or other environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION - Update these paths for your environment =====\n",
    "\n",
    "# Path to the DSL module directory\n",
    "DSL_MODULE_PATH = \"./arc-dsl\"\n",
    "\n",
    "# Path to ARC-AGI-2 data\n",
    "ARC_DATA_PATH = \"./ARC-AGI-2-main/data\"\n",
    "TRAINING_DATA_PATH = f\"{ARC_DATA_PATH}/training\"\n",
    "EVALUATION_DATA_PATH = f\"{ARC_DATA_PATH}/evaluation\"\n",
    "\n",
    "# LLM Configuration\n",
    "# Use a smaller model that fits in P100 GPU (16GB)\n",
    "LLM_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # ~7.5GB, good reasoning\n",
    "# Alternative models:\n",
    "# \"meta-llama/Llama-2-7b-chat-hf\"  # Requires Hugging Face token\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.2\"  # Good general purpose\n",
    "# \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Very small, faster\n",
    "\n",
    "USE_GPU = True  # Set to False if no GPU available\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0.1  # Lower for more deterministic outputs\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Uncomment the following lines if running on Kaggle or fresh environment\n",
    "# !pip install transformers torch accelerate bitsandbytes matplotlib numpy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "print(\"Basic imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DSL Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add DSL module to path\n",
    "if DSL_MODULE_PATH not in sys.path:\n",
    "    sys.path.insert(0, DSL_MODULE_PATH)\n",
    "\n",
    "# Import DSL modules\n",
    "try:\n",
    "    import dsl\n",
    "    import constants\n",
    "    import arc_types\n",
    "    import solvers\n",
    "    print(\"\u2713 DSL modules loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error loading DSL modules: {e}\")\n",
    "    print(\"Please ensure DSL_MODULE_PATH is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract DSL Function Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_dsl_function_catalog():\n",
    "    \"\"\"Extract all DSL functions with their signatures and docstrings.\"\"\"\n",
    "    catalog = {}\n",
    "    \n",
    "    for name, obj in inspect.getmembers(dsl):\n",
    "        if inspect.isfunction(obj) and not name.startswith('_'):\n",
    "            sig = str(inspect.signature(obj))\n",
    "            doc = inspect.getdoc(obj) or \"No description available\"\n",
    "            catalog[name] = {\n",
    "                'signature': sig,\n",
    "                'docstring': doc,\n",
    "                'full_name': f\"{name}{sig}\"\n",
    "            }\n",
    "    \n",
    "    return catalog\n",
    "\n",
    "def get_solver_catalog():\n",
    "    \"\"\"Extract all pre-built solver functions.\"\"\"\n",
    "    solver_catalog = {}\n",
    "    \n",
    "    for name, obj in inspect.getmembers(solvers):\n",
    "        if inspect.isfunction(obj) and name.startswith('solve_'):\n",
    "            task_id = name.replace('solve_', '')\n",
    "            source = inspect.getsource(obj)\n",
    "            solver_catalog[task_id] = {\n",
    "                'name': name,\n",
    "                'source': source\n",
    "            }\n",
    "    \n",
    "    return solver_catalog\n",
    "\n",
    "# Build catalogs\n",
    "dsl_catalog = get_dsl_function_catalog()\n",
    "solver_catalog = get_solver_catalog()\n",
    "\n",
    "print(f\"\u2713 Found {len(dsl_catalog)} DSL functions\")\n",
    "print(f\"\u2713 Found {len(solver_catalog)} pre-built solvers\")\n",
    "\n",
    "# Show sample DSL functions\n",
    "print(\"\\nSample DSL functions:\")\n",
    "for i, (name, info) in enumerate(list(dsl_catalog.items())[:10]):\n",
    "    print(f\"  {name}: {info['docstring'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ARC Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arc_tasks(data_path: str, limit: int = None) -> Dict[str, Dict]:\n",
    "    \"\"\"Load ARC tasks from JSON files.\"\"\"\n",
    "    tasks = {}\n",
    "    json_files = list(Path(data_path).glob(\"*.json\"))\n",
    "    \n",
    "    if limit:\n",
    "        json_files = json_files[:limit]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        task_id = json_file.stem\n",
    "        with open(json_file, 'r') as f:\n",
    "            tasks[task_id] = json.load(f)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "def visualize_task(task_data: Dict, max_examples: int = 3):\n",
    "    \"\"\"Visualize ARC task examples using matplotlib.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.colors as mcolors\n",
    "        import numpy as np\n",
    "    except ImportError:\n",
    "        print(\"matplotlib not available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # ARC color palette (0-9)\n",
    "    arc_colors = [\n",
    "        '#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'\n",
    "    ]\n",
    "    cmap = mcolors.ListedColormap(arc_colors)\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=9)\n",
    "    \n",
    "    # Plot training examples\n",
    "    train_examples = task_data.get('train', [])[:max_examples]\n",
    "    n_examples = len(train_examples)\n",
    "    \n",
    "    if n_examples == 0:\n",
    "        print(\"No training examples to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(n_examples, 2, figsize=(8, 3*n_examples))\n",
    "    if n_examples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, example in enumerate(train_examples):\n",
    "        # Input\n",
    "        axes[i][0].imshow(np.array(example['input']), cmap=cmap, norm=norm)\n",
    "        axes[i][0].set_title(f'Training Example {i+1} - Input')\n",
    "        axes[i][0].axis('off')\n",
    "        axes[i][0].grid(True, which='both', color='lightgray', linewidth=0.5)\n",
    "        \n",
    "        # Output\n",
    "        axes[i][1].imshow(np.array(example['output']), cmap=cmap, norm=norm)\n",
    "        axes[i][1].set_title(f'Training Example {i+1} - Output')\n",
    "        axes[i][1].axis('off')\n",
    "        axes[i][1].grid(True, which='both', color='lightgray', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load sample tasks\n",
    "print(\"Loading training tasks...\")\n",
    "training_tasks = load_arc_tasks(TRAINING_DATA_PATH, limit=10)\n",
    "print(f\"\u2713 Loaded {len(training_tasks)} training tasks\")\n",
    "\n",
    "# Show a sample task\n",
    "if training_tasks:\n",
    "    sample_task_id = list(training_tasks.keys())[0]\n",
    "    print(f\"\\nSample task: {sample_task_id}\")\n",
    "    sample_task = training_tasks[sample_task_id]\n",
    "    print(f\"  Train examples: {len(sample_task['train'])}\")\n",
    "    print(f\"  Test examples: {len(sample_task['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first task\n",
    "if training_tasks:\n",
    "    task_id = list(training_tasks.keys())[0]\n",
    "    print(f\"Visualizing task: {task_id}\")\n",
    "    visualize_task(training_tasks[task_id], max_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "    import torch\n",
    "    \n",
    "    print(f\"Loading model: {LLM_MODEL_NAME}...\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    device = \"cuda\" if USE_GPU and torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Load tokenizer and model with memory optimization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME, trust_remote_code=True)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    if device == \"cpu\":\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Create pipeline\n",
    "    llm_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "        do_sample=True if TEMPERATURE > 0 else False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(\"\u2713 LLM initialized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LLM: {e}\")\n",
    "    print(\"\\nYou can continue with a mock LLM for testing purposes.\")\n",
    "    llm_pipeline = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSL Program Analysis and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dsl_catalog_for_prompt(catalog: Dict, max_functions: int = 50) -> str:\n",
    "    \"\"\"Format DSL catalog for LLM prompt.\"\"\"\n",
    "    lines = [\"Available DSL Functions:\"]\n",
    "    \n",
    "    for i, (name, info) in enumerate(list(catalog.items())[:max_functions]):\n",
    "        lines.append(f\"{i+1}. {name}: {info['docstring']}\")\n",
    "    \n",
    "    if len(catalog) > max_functions:\n",
    "        lines.append(f\"... and {len(catalog) - max_functions} more functions\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def format_task_for_prompt(task_data: Dict) -> str:\n",
    "    \"\"\"Format ARC task for LLM prompt.\"\"\"\n",
    "    lines = [\"ARC Task:\"]\n",
    "    \n",
    "    # Training examples\n",
    "    lines.append(f\"\\nTraining Examples ({len(task_data['train'])}):\")\n",
    "    for i, example in enumerate(task_data['train'][:3]):  # Limit to first 3\n",
    "        lines.append(f\"\\nExample {i+1}:\")\n",
    "        lines.append(f\"Input shape: {len(example['input'])}x{len(example['input'][0])}\")\n",
    "        lines.append(f\"Output shape: {len(example['output'])}x{len(example['output'][0])}\")\n",
    "        lines.append(f\"Input: {example['input'][:3]}...\")  # Show first 3 rows\n",
    "        lines.append(f\"Output: {example['output'][:3]}...\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def create_analysis_prompt(task_data: Dict, dsl_catalog: Dict) -> str:\n",
    "    \"\"\"Create prompt for LLM to analyze task and suggest DSL operations.\"\"\"\n",
    "    prompt = f\"\"\"You are an expert in solving ARC (Abstraction and Reasoning Corpus) tasks using DSL operations.\n",
    "\n",
    "{format_task_for_prompt(task_data)}\n",
    "\n",
    "{format_dsl_catalog_for_prompt(dsl_catalog, max_functions=30)}\n",
    "\n",
    "Analyze the transformation pattern in the training examples above.\n",
    "\n",
    "Think step by step:\n",
    "1. What visual patterns do you see in the inputs?\n",
    "2. How do the outputs differ from the inputs?\n",
    "3. What DSL operations would achieve this transformation?\n",
    "\n",
    "Provide a sequence of DSL function calls that would solve this task.\n",
    "Format your answer as Python code that takes input grid I and returns output grid O.\n",
    "\n",
    "Example format:\n",
    "```python\n",
    "def solve(I):\n",
    "    # Step 1: operation\n",
    "    x1 = operation1(I)\n",
    "    # Step 2: another operation\n",
    "    O = operation2(x1)\n",
    "    return O\n",
    "```\n",
    "\n",
    "Your solution:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def query_llm(prompt: str) -> str:\n",
    "    \"\"\"Query the LLM with a prompt.\"\"\"\n",
    "    if llm_pipeline is None:\n",
    "        return \"[Mock LLM Response: Unable to generate solution without initialized LLM]\"\n",
    "    \n",
    "    try:\n",
    "        result = llm_pipeline(prompt, max_new_tokens=MAX_NEW_TOKENS)[0]['generated_text']\n",
    "        # Extract only the generated part (after the prompt)\n",
    "        response = result[len(prompt):].strip()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"[Error querying LLM: {e}]\"\n",
    "\n",
    "print(\"\u2713 Analysis functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Execute DSL Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_python_code(llm_response: str) -> str:\n",
    "    \"\"\"Extract Python code from LLM response.\"\"\"\n",
    "    # Try to find code in markdown code blocks\n",
    "    code_blocks = re.findall(r'```python\\n(.*?)```', llm_response, re.DOTALL)\n",
    "    if code_blocks:\n",
    "        return code_blocks[0].strip()\n",
    "    \n",
    "    # Try to find code without markdown\n",
    "    code_blocks = re.findall(r'```\\n(.*?)```', llm_response, re.DOTALL)\n",
    "    if code_blocks:\n",
    "        return code_blocks[0].strip()\n",
    "    \n",
    "    # If no code blocks, try to extract def solve\n",
    "    match = re.search(r'(def solve.*?return \\w+)', llm_response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def execute_dsl_code(code: str, input_grid: List[List[int]]) -> Any:\n",
    "    \"\"\"Execute DSL code on an input grid.\"\"\"\n",
    "    if not code:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Convert input to tuple format (as required by DSL)\n",
    "        I = tuple(tuple(row) for row in input_grid)\n",
    "        \n",
    "        # Create execution namespace with DSL functions\n",
    "        namespace = {\n",
    "            '__builtins__': __builtins__,\n",
    "            'I': I,\n",
    "        }\n",
    "        \n",
    "        # Add all DSL functions to namespace\n",
    "        for name in dir(dsl):\n",
    "            if not name.startswith('_'):\n",
    "                namespace[name] = getattr(dsl, name)\n",
    "        \n",
    "        # Add all constants\n",
    "        for name in dir(constants):\n",
    "            if not name.startswith('_'):\n",
    "                namespace[name] = getattr(constants, name)\n",
    "        \n",
    "        # Execute the code\n",
    "        exec(code, namespace)\n",
    "        \n",
    "        # Call the solve function\n",
    "        if 'solve' in namespace:\n",
    "            result = namespace['solve'](I)\n",
    "            # Convert back to list format\n",
    "            if isinstance(result, tuple):\n",
    "                return [list(row) for row in result]\n",
    "            return result\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error executing code: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_solution(predicted: Any, expected: List[List[int]]) -> bool:\n",
    "    \"\"\"Check if predicted output matches expected output.\"\"\"\n",
    "    if predicted is None:\n",
    "        return False\n",
    "    \n",
    "    # Convert to list if needed\n",
    "    if isinstance(predicted, tuple):\n",
    "        predicted = [list(row) for row in predicted]\n",
    "    \n",
    "    return predicted == expected\n",
    "\n",
    "print(\"\u2713 Execution functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Solver Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_task_with_llm(task_id: str, task_data: Dict, verbose: bool = True) -> Dict:\n",
    "    \"\"\"Solve an ARC task using LLM-guided DSL program search.\"\"\"\n",
    "    result = {\n",
    "        'task_id': task_id,\n",
    "        'solved': False,\n",
    "        'code': None,\n",
    "        'predictions': [],\n",
    "        'accuracy': 0.0\n",
    "    }\n",
    "    \n",
    "    # Check if there's a pre-built solver\n",
    "    if task_id in solver_catalog:\n",
    "        if verbose:\n",
    "            print(f\"Found pre-built solver for {task_id}\")\n",
    "        code = solver_catalog[task_id]['source']\n",
    "        # Extract just the function body\n",
    "        result['code'] = code\n",
    "        result['using_prebuilt'] = True\n",
    "    else:\n",
    "        # Use LLM to generate solution\n",
    "        if verbose:\n",
    "            print(f\"Analyzing task {task_id} with LLM...\")\n",
    "        \n",
    "        prompt = create_analysis_prompt(task_data, dsl_catalog)\n",
    "        llm_response = query_llm(prompt)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"LLM Response:\\n{llm_response[:500]}...\\n\")\n",
    "        \n",
    "        code = extract_python_code(llm_response)\n",
    "        result['code'] = code\n",
    "        result['llm_response'] = llm_response\n",
    "        result['using_prebuilt'] = False\n",
    "    \n",
    "    if not code:\n",
    "        if verbose:\n",
    "            print(\"No code generated\")\n",
    "        return result\n",
    "    \n",
    "    # Test on training examples\n",
    "    correct = 0\n",
    "    total = len(task_data['train'])\n",
    "    \n",
    "    for i, example in enumerate(task_data['train']):\n",
    "        predicted = execute_dsl_code(code, example['input'])\n",
    "        is_correct = evaluate_solution(predicted, example['output'])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Training example {i+1}: {'\u2713' if is_correct else '\u2717'}\")\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "    \n",
    "    result['accuracy'] = correct / total if total > 0 else 0\n",
    "    result['solved'] = (correct == total)\n",
    "    \n",
    "    # Generate predictions for test examples\n",
    "    for example in task_data['test']:\n",
    "        predicted = execute_dsl_code(code, example['input'])\n",
    "        result['predictions'].append(predicted)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Task {task_id}: {'SOLVED' if result['solved'] else 'FAILED'} ({correct}/{total})\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"\u2713 Main solver pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Sample Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few tasks\n",
    "num_tasks_to_test = 3\n",
    "results = []\n",
    "\n",
    "for task_id in list(training_tasks.keys())[:num_tasks_to_test]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing task: {task_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = solve_task_with_llm(task_id, training_tasks[task_id], verbose=True)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Visualize result\n",
    "    if result['predictions']:\n",
    "        print(f\"\\nGenerated prediction for test input\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "solved = sum(1 for r in results if r['solved'])\n",
    "print(f\"Solved: {solved}/{len(results)}\")\n",
    "avg_accuracy = sum(r['accuracy'] for r in results) / len(results) if results else 0\n",
    "print(f\"Average accuracy: {avg_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_solve_tasks(tasks: Dict[str, Dict], max_tasks: int = None) -> List[Dict]:\n",
    "    \"\"\"Solve multiple ARC tasks in batch.\"\"\"\n",
    "    results = []\n",
    "    task_ids = list(tasks.keys())[:max_tasks] if max_tasks else list(tasks.keys())\n",
    "    \n",
    "    for i, task_id in enumerate(task_ids):\n",
    "        print(f\"\\nProcessing task {i+1}/{len(task_ids)}: {task_id}\")\n",
    "        result = solve_task_with_llm(task_id, tasks[task_id], verbose=False)\n",
    "        results.append(result)\n",
    "        print(f\"  Result: {'SOLVED' if result['solved'] else 'FAILED'} (accuracy: {result['accuracy']:.2%})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_submission(results: List[Dict], output_path: str = \"submission.json\"):\n",
    "    \"\"\"Generate submission file from results.\"\"\"\n",
    "    submission = {}\n",
    "    \n",
    "    for result in results:\n",
    "        task_id = result['task_id']\n",
    "        predictions = result['predictions']\n",
    "        \n",
    "        # Format predictions for submission\n",
    "        submission[task_id] = [\n",
    "            {'attempt_1': pred, 'attempt_2': pred} \n",
    "            for pred in predictions\n",
    "        ]\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "    \n",
    "    print(f\"\u2713 Submission saved to {output_path}\")\n",
    "\n",
    "print(\"\u2713 Batch processing functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Pipeline (Optional)\n",
    "\n",
    "Uncomment and run to process all tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all training tasks\n",
    "# all_training_tasks = load_arc_tasks(TRAINING_DATA_PATH)\n",
    "# print(f\"Loaded {len(all_training_tasks)} training tasks\")\n",
    "\n",
    "# # Process tasks (limit for testing)\n",
    "# all_results = batch_solve_tasks(all_training_tasks, max_tasks=20)\n",
    "\n",
    "# # Statistics\n",
    "# solved = sum(1 for r in all_results if r['solved'])\n",
    "# total = len(all_results)\n",
    "# avg_acc = sum(r['accuracy'] for r in all_results) / total\n",
    "\n",
    "# print(f\"\\nFinal Statistics:\")\n",
    "# print(f\"  Solved: {solved}/{total} ({solved/total:.2%})\")\n",
    "# print(f\"  Average accuracy: {avg_acc:.2%}\")\n",
    "\n",
    "# # Generate submission for evaluation set\n",
    "# # eval_tasks = load_arc_tasks(EVALUATION_DATA_PATH)\n",
    "# # eval_results = batch_solve_tasks(eval_tasks)\n",
    "# # generate_submission(eval_results, \"arc_submission.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training-Based DSL Program Synthesis\n",
    "\n",
    "This section implements a training-based approach that:\n",
    "1. Searches through DSL program combinations\n",
    "2. Validates programs against training examples\n",
    "3. Selects the best program for each task\n",
    "4. Applies learned programs to test cases\n",
    "\n",
    "This is more systematic than LLM generation and provides better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSLProgramSynthesizer:\n",
    "    \"\"\"Synthesizes DSL programs by searching through combinations.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.dsl_functions = self._extract_dsl_functions()\n",
    "        self.simple_transforms = self._get_simple_transforms()\n",
    "        \n",
    "    def _extract_dsl_functions(self):\n",
    "        \"\"\"Extract all DSL functions.\"\"\"\n",
    "        functions = {}\n",
    "        for name in dir(dsl):\n",
    "            if not name.startswith('_'):\n",
    "                obj = getattr(dsl, name)\n",
    "                if callable(obj):\n",
    "                    functions[name] = obj\n",
    "        return functions\n",
    "    \n",
    "    def _get_simple_transforms(self):\n",
    "        \"\"\"Get list of simple grid transformation functions.\"\"\"\n",
    "        transforms = [\n",
    "            'identity',\n",
    "            'vmirror', 'hmirror', 'dmirror', 'cmirror',\n",
    "            'rot90', 'rot180', 'rot270',\n",
    "            'upscale', 'downscale',\n",
    "            'trim', 'compress',\n",
    "        ]\n",
    "        return [t for t in transforms if t in self.dsl_functions]\n",
    "    \n",
    "    def _try_simple_transform(self, train_examples):\n",
    "        \"\"\"Try simple single-function transformations.\"\"\"\n",
    "        for func_name in self.simple_transforms:\n",
    "            func = self.dsl_functions[func_name]\n",
    "            \n",
    "            all_match = True\n",
    "            for example in train_examples:\n",
    "                try:\n",
    "                    I = tuple(tuple(row) for row in example['input'])\n",
    "                    expected = tuple(tuple(row) for row in example['output'])\n",
    "                    \n",
    "                    result = func(I)\n",
    "                    if result != expected:\n",
    "                        all_match = False\n",
    "                        break\n",
    "                except:\n",
    "                    all_match = False\n",
    "                    break\n",
    "            \n",
    "            if all_match:\n",
    "                return f\"def solve(I):\\n    O = {func_name}(I)\\n    return O\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _try_prebuilt_solver(self, task_id, train_examples):\n",
    "        \"\"\"Check if a pre-built solver exists and works.\"\"\"\n",
    "        import inspect\n",
    "        solver_name = f\"solve_{task_id}\"\n",
    "        \n",
    "        if hasattr(solvers, solver_name):\n",
    "            solver_func = getattr(solvers, solver_name)\n",
    "            \n",
    "            all_match = True\n",
    "            for example in train_examples:\n",
    "                try:\n",
    "                    I = tuple(tuple(row) for row in example['input'])\n",
    "                    expected = tuple(tuple(row) for row in example['output'])\n",
    "                    \n",
    "                    result = solver_func(I)\n",
    "                    if result != expected:\n",
    "                        all_match = False\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    all_match = False\n",
    "                    break\n",
    "            \n",
    "            if all_match:\n",
    "                source = inspect.getsource(solver_func)\n",
    "                source = source.replace(f\"def {solver_name}(\", \"def solve(\")\n",
    "                return source\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _try_combined_transforms(self, train_examples):\n",
    "        \"\"\"Try combinations of 2 simple transforms.\"\"\"\n",
    "        for func1_name in self.simple_transforms[:10]:\n",
    "            for func2_name in self.simple_transforms[:10]:\n",
    "                func1 = self.dsl_functions[func1_name]\n",
    "                func2 = self.dsl_functions[func2_name]\n",
    "                \n",
    "                all_match = True\n",
    "                for example in train_examples:\n",
    "                    try:\n",
    "                        I = tuple(tuple(row) for row in example['input'])\n",
    "                        expected = tuple(tuple(row) for row in example['output'])\n",
    "                        \n",
    "                        temp = func1(I)\n",
    "                        result = func2(temp)\n",
    "                        \n",
    "                        if result != expected:\n",
    "                            all_match = False\n",
    "                            break\n",
    "                    except:\n",
    "                        all_match = False\n",
    "                        break\n",
    "                \n",
    "                if all_match:\n",
    "                    code = f\"\"\"def solve(I):\n",
    "    x1 = {func1_name}(I)\n",
    "    O = {func2_name}(x1)\n",
    "    return O\"\"\"\n",
    "                    return code\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def synthesize_program(self, task_id, train_examples):\n",
    "        \"\"\"Synthesize a DSL program for the given task.\"\"\"\n",
    "        # Strategy 1: Check for pre-built solver\n",
    "        program = self._try_prebuilt_solver(task_id, train_examples)\n",
    "        if program:\n",
    "            return program, \"prebuilt\"\n",
    "        \n",
    "        # Strategy 2: Try simple single transforms\n",
    "        program = self._try_simple_transform(train_examples)\n",
    "        if program:\n",
    "            return program, \"simple_transform\"\n",
    "        \n",
    "        # Strategy 3: Try combined transforms\n",
    "        program = self._try_combined_transforms(train_examples)\n",
    "        if program:\n",
    "            return program, \"combined_transforms\"\n",
    "        \n",
    "        return None, \"none\"\n",
    "\n",
    "print(\"\u2713 DSL Program Synthesizer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on All Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_dsl_program(code, input_grid):\n",
    "    \"\"\"Execute a DSL program on an input grid.\"\"\"\n",
    "    if not code:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        I = tuple(tuple(row) for row in input_grid)\n",
    "        \n",
    "        namespace = {\n",
    "            '__builtins__': __builtins__,\n",
    "            'I': I,\n",
    "        }\n",
    "        \n",
    "        # Add all DSL functions\n",
    "        for name in dir(dsl):\n",
    "            if not name.startswith('_'):\n",
    "                namespace[name] = getattr(dsl, name)\n",
    "        \n",
    "        # Add all constants\n",
    "        for name in dir(constants):\n",
    "            if not name.startswith('_'):\n",
    "                namespace[name] = getattr(constants, name)\n",
    "        \n",
    "        exec(code, namespace)\n",
    "        \n",
    "        if 'solve' in namespace:\n",
    "            result = namespace['solve'](I)\n",
    "            if isinstance(result, tuple):\n",
    "                return [list(row) for row in result]\n",
    "            return result\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def train_on_all_tasks(tasks, limit=None):\n",
    "    \"\"\"Train DSL programs on all tasks.\"\"\"\n",
    "    synthesizer = DSLProgramSynthesizer()\n",
    "    \n",
    "    task_ids = list(tasks.keys())[:limit] if limit else list(tasks.keys())\n",
    "    results = {}\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    stats = {\n",
    "        'total': len(task_ids),\n",
    "        'solved': 0,\n",
    "        'methods': defaultdict(int),\n",
    "        'avg_accuracy': 0.0\n",
    "    }\n",
    "    \n",
    "    print(f\"Training on {len(task_ids)} tasks...\")\n",
    "    \n",
    "    for i, task_id in enumerate(task_ids):\n",
    "        task_data = tasks[task_id]\n",
    "        \n",
    "        # Synthesize program\n",
    "        program, method = synthesizer.synthesize_program(task_id, task_data['train'])\n",
    "        \n",
    "        # Validate on training data\n",
    "        train_correct = 0\n",
    "        for example in task_data['train']:\n",
    "            predicted = execute_dsl_program(program, example['input'])\n",
    "            if predicted == example['output']:\n",
    "                train_correct += 1\n",
    "        \n",
    "        train_accuracy = train_correct / len(task_data['train']) if task_data['train'] else 0\n",
    "        \n",
    "        # Generate test predictions\n",
    "        test_predictions = []\n",
    "        for test_example in task_data['test']:\n",
    "            predicted = execute_dsl_program(program, test_example['input'])\n",
    "            if predicted is None:\n",
    "                predicted = [[0] * len(test_example['input'][0]) for _ in range(len(test_example['input']))]\n",
    "            test_predictions.append(predicted)\n",
    "        \n",
    "        results[task_id] = {\n",
    "            'program': program,\n",
    "            'method': method,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'test_predictions': test_predictions\n",
    "        }\n",
    "        \n",
    "        # Update stats\n",
    "        if train_accuracy == 1.0:\n",
    "            stats['solved'] += 1\n",
    "        stats['methods'][method] += 1\n",
    "        stats['avg_accuracy'] += train_accuracy\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(task_ids)} tasks...\")\n",
    "    \n",
    "    stats['avg_accuracy'] /= len(task_ids)\n",
    "    \n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"  Solved: {stats['solved']}/{stats['total']} ({stats['solved']/stats['total']:.1%})\")\n",
    "    print(f\"  Average accuracy: {stats['avg_accuracy']:.1%}\")\n",
    "    print(f\"  Methods used:\")\n",
    "    for method, count in stats['methods'].items():\n",
    "        print(f\"    {method}: {count}\")\n",
    "    \n",
    "    return results, stats\n",
    "\n",
    "print(\"\u2713 Training functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training on All 1000 Tasks\n",
    "\n",
    "This cell trains DSL programs on all 1000 training tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all training tasks\n",
    "print(\"Loading all training tasks...\")\n",
    "all_training_tasks = load_arc_tasks(TRAINING_DATA_PATH)\n",
    "print(f\"Loaded {len(all_training_tasks)} training tasks\")\n",
    "\n",
    "# Train on all tasks\n",
    "trained_results, training_stats = train_on_all_tasks(all_training_tasks)\n",
    "\n",
    "# Save trained programs\n",
    "import json\n",
    "with open('trained_programs.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'results': trained_results,\n",
    "        'stats': training_stats\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n\u2713 Trained programs saved to trained_programs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation tasks\n",
    "print(\"Loading evaluation tasks...\")\n",
    "eval_tasks = load_arc_tasks(EVALUATION_DATA_PATH)\n",
    "print(f\"Loaded {len(eval_tasks)} evaluation tasks\")\n",
    "\n",
    "# Generate predictions for evaluation tasks\n",
    "eval_results = {}\n",
    "for task_id, task_data in eval_tasks.items():\n",
    "    # Check if we have a trained program for this task\n",
    "    if task_id in trained_results:\n",
    "        program = trained_results[task_id]['program']\n",
    "        predictions = []\n",
    "        \n",
    "        for test_example in task_data['test']:\n",
    "            predicted = execute_dsl_program(program, test_example['input'])\n",
    "            if predicted is None:\n",
    "                predicted = [[0] * len(test_example['input'][0]) for _ in range(len(test_example['input']))]\n",
    "            predictions.append(predicted)\n",
    "        \n",
    "        eval_results[task_id] = predictions\n",
    "    else:\n",
    "        # No trained program, return zero predictions\n",
    "        predictions = []\n",
    "        for test_example in task_data['test']:\n",
    "            predicted = [[0] * len(test_example['input'][0]) for _ in range(len(test_example['input']))]\n",
    "            predictions.append(predicted)\n",
    "        eval_results[task_id] = predictions\n",
    "\n",
    "print(f\"\\nGenerated predictions for {len(eval_results)} evaluation tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Kaggle Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file in Kaggle format\n",
    "submission = {}\n",
    "\n",
    "for task_id, predictions in eval_results.items():\n",
    "    # ARC submission format: 2 attempts per test case\n",
    "    submission[task_id] = [\n",
    "        {'attempt_1': pred, 'attempt_2': pred}\n",
    "        for pred in predictions\n",
    "    ]\n",
    "\n",
    "with open('arc_submission.json', 'w') as f:\n",
    "    json.dump(submission, f, indent=2)\n",
    "\n",
    "print(\"\u2713 Submission file saved to arc_submission.json\")\n",
    "print(f\"  Ready for Kaggle submission with {len(submission)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional DSL Programs for ARC-AGI-2\n",
    "\n",
    "Since the original DSL was designed for ARC-AGI-1, here are some additional useful programs that can be added to handle ARC-AGI-2 tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_dsl_operations():\n",
    "    \"\"\"\n",
    "    Additional DSL-style operations that might be useful for ARC-AGI-2.\n",
    "    These can be added to the dsl.py module if needed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def find_repeating_pattern(grid):\n",
    "        \"\"\"Detect repeating patterns in the grid.\"\"\"\n",
    "        # Implementation for pattern detection\n",
    "        pass\n",
    "    \n",
    "    def symmetry_detection(grid):\n",
    "        \"\"\"Detect if grid has horizontal, vertical, or diagonal symmetry.\"\"\"\n",
    "        # Implementation for symmetry detection\n",
    "        pass\n",
    "    \n",
    "    def color_frequency_analysis(grid):\n",
    "        \"\"\"Analyze frequency of each color in the grid.\"\"\"\n",
    "        from collections import Counter\n",
    "        flat = [cell for row in grid for cell in row]\n",
    "        return Counter(flat)\n",
    "    \n",
    "    def find_largest_region(grid, color):\n",
    "        \"\"\"Find the largest contiguous region of a specific color.\"\"\"\n",
    "        # Implementation for region finding\n",
    "        pass\n",
    "    \n",
    "    def grid_interpolation(grid, factor):\n",
    "        \"\"\"Interpolate grid to increase resolution.\"\"\"\n",
    "        # Implementation for interpolation\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'find_repeating_pattern': find_repeating_pattern,\n",
    "        'symmetry_detection': symmetry_detection,\n",
    "        'color_frequency_analysis': color_frequency_analysis,\n",
    "        'find_largest_region': find_largest_region,\n",
    "        'grid_interpolation': grid_interpolation,\n",
    "    }\n",
    "\n",
    "print(\"\u2713 Extended DSL operations defined\")\n",
    "print(\"\\nNote: These are template functions. Implement them as needed for specific task types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### On Kaggle:\n",
    "\n",
    "1. Upload this notebook to Kaggle\n",
    "2. Add the ARC-AGI-2 dataset as input data\n",
    "3. Update configuration paths at the top:\n",
    "   ```python\n",
    "   DSL_MODULE_PATH = \"/kaggle/input/arc-dsl/arc-dsl\"\n",
    "   ARC_DATA_PATH = \"/kaggle/input/arc-agi-2/data\"\n",
    "   ```\n",
    "4. Enable GPU accelerator (P100)\n",
    "5. Install dependencies (uncomment the pip install cell)\n",
    "6. Run all cells\n",
    "\n",
    "### Local Usage:\n",
    "\n",
    "1. Ensure you have the DSL module and ARC-AGI-2 data\n",
    "2. Update paths in configuration cell\n",
    "3. Install dependencies: `pip install transformers torch matplotlib numpy`\n",
    "4. Run cells sequentially\n",
    "\n",
    "### Customization:\n",
    "\n",
    "- Adjust `LLM_MODEL_NAME` for different models\n",
    "- Modify `MAX_NEW_TOKENS` and `TEMPERATURE` for different generation behavior\n",
    "- Extend DSL operations as needed for specific task types\n",
    "- Modify prompts in `create_analysis_prompt` for better task understanding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}